\chapter*{Abstract}

\section*{\tituloPortadaEngVal}
 
This study focuses on comparing different \gls{ai} models for detecting specific gestures using the Perception Neuron 3 motion capture suit, aiming to create interactions with \glspl{npc} through non-verbal communication and to develop tools derived from the models that enhance non-verbal communication in virtual environments.

To achieve this, public gesture datasets were first sought, but none were found that met the study's needs. Therefore, a tool was created to convert Mixamo animations into \glspl{csv} format so they could be understood by the models to be trained. This tool generated a large number of animations, but they were highly imbalanced in the number of animations for each type.

To address the imbalance issue, a series of user trials (N=65) were conducted, where participants were asked to perform three instances of each gesture while wearing the motion capture suit. This data collection resulted in a total of 975 animations, with 195 human gestures for running, greeting, pointing, punching, and sitting. The gesture of dancing was omitted from the trials due to the imbalance in the dataset generated by computer animations. This omission helped balance the number of animations but did not completely eliminate the imbalance in the standardized data.

Once the dataset was established, various \gls{ai} models for gesture detection were implemented under a web interface for ease of use. These models included: \gls{lstm}, \gls{cnn}, \gls{rnn}, and \gls{randomforest}. After training all the models, it was observed that the \gls{randomforest} model significantly outperformed the others, achieving 95\% accuracy in training and 86\% in testing. The results from the neural network-based models were considerably lower, likely due to insufficient data, limited training time, and hardware constraints that hindered better model performance.

Finally, the \gls{randomforest} model was exported to TensorFlow Serving to be used in a demonstration application featuring a reactive \gls{npc}. This application allows users to see how the \gls{npc} interprets different gestures and responds simply to them.
\section*{Keywords}

\noindent Motion capture, Unity, Artificial Intelligence, Non-verbal communication, Perception Neuron, TensorFlow, YDF, Keras, Virtual Reality



